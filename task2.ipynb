{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c82624c-3d01-4cc8-8e9c-4e89319fb080",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, BatchSampler, random_split\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import emoji\n",
    "from collections import defaultdict\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.svm import LinearSVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db56dfc4-17d3-423e-9d22-35c8a9264122",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624c7a4e-686a-4573-8903-7469399c89a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "textual_emoticons_to_spanish = {\n",
    "        \":)\": \"cara sonriente\",\n",
    "        \":(\": \"cara triste\",\n",
    "        \";)\": \"guiño\",\n",
    "        \":D\": \"cara riendo con los ojos abiertos\",\n",
    "        \"XD\": \"cara riendo con los ojos cerrados\",\n",
    "        \"xD\": \"cara riendo con los ojos cerrados\",\n",
    "        \":P\": \"cara sacando la lengua\",\n",
    "        \"<3\": \"corazón\",\n",
    "        \":'(\": \"cara llorando\",\n",
    "        \":-)\": \"cara sonriente\",\n",
    "        \":-(\": \"cara triste\",\n",
    "        \";-)\": \"guiño\",\n",
    "        \":-D\": \"cara riendo con los ojos abiertos\",\n",
    "        \":-P\": \"cara sacando la lengua\",\n",
    "        \"(heart)\": \"corazón\",\n",
    "        \":o\": \"cara sorprendida\",\n",
    "        \":-o\": \"cara sorprendida\",\n",
    "        \":/\": \"cara de duda\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ff3f93-84d7-4259-8480-e89733101101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "\n",
    "    print(text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "\n",
    "    text = re.sub(r'(ja)+', 'jaja', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'(js)+', 'jsjs', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    text = text.replace(\" @\",\"o\")\n",
    "\n",
    "    for x,y in textual_emoticons_to_spanish.items():\n",
    "        text = text.replace(x,y)\n",
    "\n",
    "    text = emoji.demojize(text,language='es')\n",
    "\n",
    "    spanishVowels = 'aeiouáéíóú' \n",
    "    uppercaseVowels =spanishVowels.upper()\n",
    "    \n",
    "    for vow in spanishVowels + uppercaseVowels:\n",
    "\n",
    "        pattern = re.compile(f\"{vow}{vow}{vow}+\")\n",
    "\n",
    "        text = pattern.sub(f'{vow}',text)\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad93f57-f497-449a-82ee-5dca51a8c684",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15fb65f-181e-4100-9327-7b142588bf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_MAP = {'none':0, 'anxiety':1, 'depression':2}\n",
    "REVERSE_LABEL_MAP = {0:'none', 1:'anxiety', 2:'depression'}\n",
    "\n",
    "CONTEXTS = ['addiction','emergency','family','work','social','other','none']\n",
    "CONTEXT_MAP = {c:i for i,c in enumerate(CONTEXTS)}\n",
    "REVERSE_CONTEXT_MAP = {i:c for i,c in enumerate(CONTEXTS)}\n",
    "\n",
    "data = {'messages':[], 'labels1':[], 'labels2':[]}\n",
    "\n",
    "\n",
    "for split in ['trial']:\n",
    "    df = pd.read_csv(f'./data/task2/{split}/gold_task2.txt')\n",
    "    labels1 = df['label'].map(lambda x: LABEL_MAP[x]).to_list()\n",
    "    data['labels1'] += labels1\n",
    "    \n",
    "    subjects = df['Subject'].to_list()\n",
    "    messages = []\n",
    "    for subject in subjects:\n",
    "        subject_data = json.load(open(f'./data/task2/{split}/subjects/{subject}.json', 'r', encoding='utf-8'))\n",
    "        messages.append([preprocess(x['message']) for x in subject_data])\n",
    "    data['messages'] += messages\n",
    "    data['labels2'].append(df[CONTEXTS].to_numpy().astype(np.int32))\n",
    "\n",
    "data['labels2'] = np.concatenate(data['labels2'], axis=0)\n",
    "data['labels1'] = np.array(data['labels1'], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54306b7-faf8-4756-8a12-23d8f36f5337",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431c3abc-8528-4638-b9dd-951e4d5a4c1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b3d143-d6e2-4df4-8d80-7d739d67f1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ignacio-ave/beto-sentiment-analysis-spanish\")\n",
    "model_seq = AutoModelForSequenceClassification.from_pretrained(\"ignacio-ave/beto-sentiment-analysis-spanish\")\n",
    "model = AutoModel.from_pretrained(\"ignacio-ave/beto-sentiment-analysis-spanish\")\n",
    "model.eval()\n",
    "model.to(device)\n",
    "model_seq.eval()\n",
    "model_seq.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2208b86f-9c84-4ded-b606-be65a0706816",
   "metadata": {},
   "outputs": [],
   "source": [
    "isTrain = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d208f976-cdbf-4f2a-8167-1faf983e4a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cls_embeddings(all_messages, model, tokenizer, device, m_length=96):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for subject_messages in tqdm(all_messages):\n",
    "            input = tokenizer(subject_messages, padding=True, truncation=True, max_length=m_length, return_tensors='pt')\n",
    "            output = model(**input.to(device))\n",
    "            embeddings.append(output.last_hidden_state[:, 0, :].cpu().numpy())\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99795b7-e8ee-4c01-9b7b-12fb49288116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_rnn(net, device, criterion, val_dl):\n",
    "    net.to(device)\n",
    "    net.eval()\n",
    "    loss, num_batches = 0, 0\n",
    "    preds, labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_lens, batch_labels in val_dl:\n",
    "            labels.append(batch_labels.numpy())\n",
    "            \n",
    "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device).long()\n",
    "            batch_labels = batch_labels.to(torch.float)\n",
    "            \n",
    "            batch_lens = batch_lens.to(device)\n",
    "            out = net(batch_data, batch_lens)\n",
    "            batch_loss = criterion(out, batch_labels)\n",
    "    \n",
    "            loss += batch_loss.sum()\n",
    "            num_batches += 1\n",
    "            out = out.detach().cpu().numpy()\n",
    "            batch_predictions = (out>=0.5).astype(int)\n",
    "        \n",
    "            \n",
    "            preds.append(batch_predictions)\n",
    "            \n",
    "  \n",
    "   \n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "\n",
    "    preds = np.concatenate(preds, axis=0)\n",
    "    \n",
    " \n",
    "    loss = loss / num_batches\n",
    "\n",
    "    reports = []\n",
    "\n",
    "    for i in range(len(CONTEXTS)):\n",
    "\n",
    "        #print(labels[:,i])\n",
    "        #print(preds[:,i])\n",
    "        report = {\n",
    "            'loss': loss,\n",
    "            'acc': metrics.accuracy_score(labels[:,i], preds[:,i]),\n",
    "            'macro_f1': metrics.f1_score(labels[:,i], preds[:,i], average='macro', zero_division=0),\n",
    "            'macro_precision': metrics.precision_score(labels[:,i], preds[:,i], average='macro', zero_division=0),\n",
    "            'macro_recall': metrics.recall_score(labels[:,i], preds[:,i], average='macro', zero_division=0),\n",
    "            'micro_f1': metrics.f1_score(labels[:,i], preds[:,i], average='micro', zero_division=0),\n",
    "            'micro_precision': metrics.precision_score(labels[:,i], preds[:,i], average='micro', zero_division=0),\n",
    "            'micro_recall': metrics.recall_score(labels[:,i], preds[:,i], average='micro', zero_division=0),\n",
    "            'cls_report': metrics.classification_report(labels[:,i], preds[:,i], zero_division=0),\n",
    "            'cfm': metrics.confusion_matrix(labels[:,i], preds[:,i])\n",
    "        }\n",
    "\n",
    "        #print(report)\n",
    "        reports.append(report)\n",
    "\n",
    "    #print(reports)\n",
    "    return reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef6a70d-2111-4f69-aa09-a48fac8d8f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gdro_rnn(net, optimizer, device, criterion, train_dl, q, eta=0.1):\n",
    "    net.to(device)\n",
    "    net.train()\n",
    "    loss = 0\n",
    "    num_batches = 0\n",
    "    preds = []\n",
    "    labels = []\n",
    "\n",
    "    for batch_data, batch_lens, batch_labels in train_dl:\n",
    "        \n",
    "        labels.append(batch_labels.numpy())\n",
    "        unique_batch_labels = np.unique(batch_labels.numpy())\n",
    "        batch_labels = batch_labels.to(torch.float)\n",
    "    \n",
    "        batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
    "        batch_lens = batch_lens.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        out = net(batch_data, batch_lens)\n",
    "\n",
    "        batch_losses = criterion(out, batch_labels)\n",
    "        \n",
    "        batch_losses.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss += batch_losses.sum()\n",
    "      \n",
    "        num_batches += 1\n",
    "        out = out.detach().cpu().numpy()\n",
    "        batch_predictions = (out>=0.5).astype(int)\n",
    "        preds.append(batch_predictions)\n",
    "        \n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "    preds = np.concatenate(preds, axis=0)\n",
    "    loss = loss / num_batches\n",
    "    \n",
    "    reports = []\n",
    "\n",
    "    for i in range(len(CONTEXTS)):\n",
    "        report = {\n",
    "            'loss': loss,\n",
    "            'acc': metrics.accuracy_score(labels[:,i], preds[:,i]),\n",
    "            'macro_f1': metrics.f1_score(labels[:,i], preds[:,i], average='macro', zero_division=0),\n",
    "            'macro_precision': metrics.precision_score(labels[:,i], preds[:,i], average='macro', zero_division=0),\n",
    "            'macro_recall': metrics.recall_score(labels[:,i], preds[:,i], average='macro', zero_division=0),\n",
    "            'micro_f1': metrics.f1_score(labels[:,i], preds[:,i], average='micro', zero_division=0),\n",
    "            'micro_precision': metrics.precision_score(labels[:,i], preds[:,i], average='micro', zero_division=0),\n",
    "            'micro_recall': metrics.recall_score(labels[:,i], preds[:,i], average='micro', zero_division=0),\n",
    "            'cls_report': metrics.classification_report(labels[:,i], preds[:,i], zero_division=0),\n",
    "            'cfm': metrics.confusion_matrix(labels[:,i], preds[:,i])\n",
    "        }\n",
    "        reports.append(report)\n",
    "    return reports, q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a01cf0-0b8e-46bb-ae2d-d4b6f70290c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train_gdro_rnn(net, optimizer, criterion, device, train_dl, val_dl, max_epochs,n_classes):\n",
    "    num_epochs_ni = 0\n",
    "    best_vacc = 0\n",
    "\n",
    "    D = dict()\n",
    "    D2 = dict()\n",
    "    for i in range(len(CONTEXTS)):\n",
    "        D[i] = defaultdict(list)\n",
    "        D2[i] = defaultdict(list)\n",
    "    logs = {'train':D, 'val':D2}\n",
    "    q = torch.ones(n_classes, dtype=torch.float32, device=device) / n_classes\n",
    "    \n",
    "    for epoch in tqdm(range(max_epochs)):\n",
    "        \n",
    "        train_report, q = train_gdro_rnn(net, optimizer, device, criterion, train_dl, q=q)\n",
    "\n",
    "        for i in range(len(CONTEXTS)):\n",
    "            for k in train_report[i].keys():\n",
    "                logs['train'][i][k].append(train_report[i][k])\n",
    "\n",
    "        val_report = validate_rnn(net, device, criterion, val_dl)\n",
    "\n",
    "        for i in range(len(CONTEXTS)):\n",
    "            for k in val_report[i].keys():\n",
    "                logs['val'][i][k].append(val_report[i][k])\n",
    "    return logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c732758e-8ecd-4092-88bd-f6388eba59a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbDatasetRNN(Dataset):\n",
    "    '''\n",
    "    For samples in class 'none' avg embeddings from a random number of messages\n",
    "    '''\n",
    "    def __init__(self, embeddings, labels,isTrain):\n",
    "        self.embeddings = embeddings\n",
    "        self.labels = labels\n",
    "        self.isTrain = isTrain\n",
    "        self.mappingDict = dict()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        #print(self.isTrain)\n",
    "        if self.isTrain:\n",
    "            if self.labels[idx][-1]==1:\n",
    "                return self.embeddings[idx], self.labels[idx]\n",
    "            else:\n",
    "\n",
    "                p = random.random()\n",
    "\n",
    "                if p<0.6:\n",
    "                    x = None \n",
    "                    \n",
    "                    if False and idx in self.mappingDict.keys():\n",
    "                        x = self.mappingDict[idx]\n",
    "                    else:\n",
    "                        x = random.randint(0,len(self.embeddings)-1)\n",
    "        \n",
    "                        if x==idx:\n",
    "                            x = random.randint(0,len(self.embeddings)-1)\n",
    "    \n",
    "                        self.mappingDict[idx] = x\n",
    "                    return np.concatenate([self.embeddings[x],self.embeddings[idx]],axis=0), np.concatenate([self.labels[idx][:-1] | self.labels[x][-1], self.labels[idx][-1:] & self.labels[x][-1:]],axis=0)\n",
    "                else:\n",
    "                    return self.embeddings[idx], self.labels[idx]\n",
    "        return self.embeddings[idx], self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c527ddae-3f7f-44e1-b067-b3b0c91bd305",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, h_size, output_dim, dropout=0):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.h_size = h_size\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=h_size, num_layers=1, batch_first=False,\n",
    "                           dropout=dropout, bidirectional=False)\n",
    "        self.classifier = nn.Linear(h_size, output_dim)\n",
    "\n",
    "    def forward(self, seq_data, seq_lens, state=None):\n",
    "        # seq_data : (S, N, input_size)\n",
    "        # seq_lens: (N,) -> numbers between 0 and S-1 -> position of last actual sample before padding\n",
    "        if state is None:\n",
    "            state = (\n",
    "                torch.zeros(1, seq_data.shape[1], self.h_size, device=seq_data.device),\n",
    "                torch.zeros(1, seq_data.shape[1], self.h_size, device=seq_data.device)\n",
    "            )\n",
    "\n",
    "        out_states, _ = self.lstm(seq_data, state) # S, N, H\n",
    "        pred_states = torch.take_along_dim(out_states, seq_lens, dim=0).squeeze() # remove seq dimension\n",
    "        out = self.classifier(pred_states)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9b142d-25a1-4429-850e-83c51cd5d793",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def lstm_collate(batch):\n",
    "\n",
    "    labels = [x[1] for x in batch]\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    " \n",
    "    data = [torch.tensor(x[0], dtype=torch.float32) for x in batch]\n",
    "    batch_data = pad_sequence(data)\n",
    "    \n",
    "    lens = torch.tensor([len(x) for x in data], dtype=torch.long).unsqueeze(0).unsqueeze(-1) # 1, N, 1\n",
    "\n",
    "    lens -= 1\n",
    "    return batch_data, lens, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff484b7-847d-4b2e-848b-7a8b6b2ea598",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plot(train_scores, val_scores, y_label, figsize=(8,5)):\n",
    "    fig, ax = plt.subplots(1,1,figsize=figsize)\n",
    "    ax.plot(train_scores, label='Train')\n",
    "    ax.plot(val_scores, label='Val')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel(y_label)\n",
    "    ax.legend()\n",
    "\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ebcf5c-8b52-40d9-be54-38db8ce39a93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e549bbf-faaa-4891-89f0-9349f8131312",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c06f6e2-e0f4-433c-a658-28639290c2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "model_names = [\n",
    "    'ignacio-ave/beto-sentiment-analysis-spanish',    \n",
    "]\n",
    "\n",
    "labels2 = data['labels2'][data['labels1']!=0]\n",
    "\n",
    "messages2 = [x for i,x in enumerate(data['messages']) if data['labels1'][i] != 0]\n",
    "\n",
    "class_positive_weight = torch.tensor(data['labels2'].shape[0] / np.sum(data['labels2'], axis=0), device=device, dtype=torch.float32)\n",
    "\n",
    "for name in model_names:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "    model = AutoModel.from_pretrained(name)\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    embs = get_cls_embeddings(messages2, model, tokenizer, device)\n",
    "    ds = EmbDatasetRNN(embs, labels2, True)\n",
    "\n",
    "    train_ds, test_ds = random_split(ds, [0.8, 0.2], generator=torch.Generator().manual_seed(1007))\n",
    "\n",
    "    embds = []\n",
    "    labels = []\n",
    "    for x,y in train_ds:\n",
    "        embds.append(x)\n",
    "        labels.append(y)\n",
    "\n",
    "    add_embds = []\n",
    "    add_labels = []\n",
    "\n",
    "    for i in range(2):\n",
    "        for x,lx in zip(embds,labels):\n",
    "            y = random.randint(0,len(embds)-1)\n",
    "            add_embds.append(np.concatenate([np.array(embds[y]),np.array(x)],axis=0))\n",
    "            add_labels.append(lx | labels[y])\n",
    "\n",
    "    train_ds = EmbDatasetRNN(embds,labels,True)\n",
    "\n",
    "\n",
    "    embds = []\n",
    "    labels = []\n",
    "    \n",
    "    for x,y in test_ds:\n",
    "        embds.append(x)\n",
    "        labels.append(y)\n",
    "\n",
    "    test_ds = EmbDatasetRNN(embds,labels,False)\n",
    "        \n",
    "    train_dl = DataLoader(train_ds, batch_size=64, shuffle=True, drop_last=False,collate_fn=lstm_collate)\n",
    " \n",
    "    test_dl = DataLoader(test_ds, batch_size=128, shuffle=False, drop_last=False,collate_fn=lstm_collate)\n",
    "    \n",
    "    net = LSTMClassifier(embs[0].shape[-1], h_size=128, output_dim=len(CONTEXTS))\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=1e-4)\n",
    "    loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_positive_weight)\n",
    "    \n",
    "    logs = run_train_gdro_rnn(net,optimizer,loss_fn,device, train_dl, test_dl, 100,len(CONTEXTS))\n",
    "\n",
    "    \n",
    "    for i in range(len(CONTEXTS)):\n",
    "        arg = np.argmax(logs['val'][i]['macro_f1'])\n",
    "        print(f\"Val macro_f1: {logs['val'][i]['macro_f1'][arg]:.4f} | Train macro_f1: {logs['train'][i]['macro_f1'][arg]:.4f}\")\n",
    "        print('Val', logs['val'][i]['cfm'][arg], logs['val'][i]['cls_report'][arg], sep='\\n')\n",
    "        print('Train', logs['train'][i]['cfm'][arg], logs['train'][i]['cls_report'][arg], sep='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670d0241-7f36-488c-8f61-bee861aeb46f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306f1739-3fd5-4a92-b629-e7616c248445",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733daefa-97e5-4708-ab95-9ad13f4fa320",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71523ff-7f6e-4863-a207-3e836cb0f057",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3821ae86-1c32-4295-a97d-d54ba086bb9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde2ef7e-6b49-4559-a6eb-1210940eb4e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e2c836-137c-435d-96c2-60b89815e864",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76619756-fd93-4ea7-a161-12e12d2b97c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
