{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca06cf93-2880-4f83-861a-47396723267b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, BatchSampler, random_split\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "import re\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d0139a-8bdc-41f1-aa5a-98c5191fffe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbDataset(Dataset):\n",
    "    def __init__(self, embeddings, labels):\n",
    "        self.embeddings = embeddings\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.embeddings[idx], self.labels[idx]\n",
    "\n",
    "def validate(net, device, criterion, val_dl):\n",
    "    net.to(device)\n",
    "    net.train()\n",
    "    loss = 0\n",
    "    num_batches = 0\n",
    "    preds = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (batch_data, batch_labels) in enumerate(val_dl):\n",
    "            labels.append(batch_labels.numpy())\n",
    "            \n",
    "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device).long()\n",
    "            out = net(batch_data)\n",
    "            batch_loss = criterion(out, batch_labels)\n",
    "    \n",
    "            loss += batch_loss.item()\n",
    "            num_batches += 1\n",
    "            batch_predictions = torch.argmax(out, axis=-1)\n",
    "            preds.append(batch_predictions.cpu().numpy())\n",
    "\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "    preds = np.concatenate(preds, axis=0)\n",
    "    loss = loss / num_batches\n",
    "    report = {\n",
    "        'loss': loss,\n",
    "        'acc': metrics.accuracy_score(labels, preds),\n",
    "        'macro_f1': metrics.f1_score(labels, preds, average='macro', zero_division=0),\n",
    "        'macro_precision': metrics.precision_score(labels, preds, average='macro', zero_division=0),\n",
    "        'macro_recall': metrics.recall_score(labels, preds, average='macro', zero_division=0),\n",
    "        'micro_f1': metrics.f1_score(labels, preds, average='micro', zero_division=0),\n",
    "        'micro_precision': metrics.precision_score(labels, preds, average='micro', zero_division=0),\n",
    "        'micro_recall': metrics.recall_score(labels, preds, average='micro', zero_division=0),\n",
    "        'cls_report': metrics.classification_report(labels, preds, zero_division=0),\n",
    "        'cfm': metrics.confusion_matrix(labels, preds)\n",
    "    }\n",
    "    return report\n",
    "\n",
    "def train_gdro(net, optimizer, device, criterion, train_dl, q, eta=0.1):\n",
    "    net.to(device)\n",
    "    net.train()\n",
    "    loss = 0\n",
    "    num_batches = 0\n",
    "    preds = []\n",
    "    labels = []\n",
    "\n",
    "    for i, (batch_data, batch_labels) in enumerate(train_dl):\n",
    "        labels.append(batch_labels.numpy())\n",
    "        \n",
    "        batch_data, batch_labels = batch_data.to(device), batch_labels.to(device).long()\n",
    "        optimizer.zero_grad()\n",
    "        out = net(batch_data)\n",
    "        batch_losses = F.cross_entropy(out, batch_labels, reduction='none')\n",
    "\n",
    "        ## compute loss here\n",
    "        for cls in torch.unique(batch_labels):\n",
    "            idx_cls = batch_labels == cls\n",
    "            q[cls] *= (eta * batch_losses[idx_cls].mean()).exp().item()\n",
    "\n",
    "        q /= q.sum()\n",
    "\n",
    "        loss_value = 0\n",
    "        for cls in torch.unique(batch_labels):\n",
    "            idx_cls = batch_labels == cls\n",
    "            loss_value += q[cls] * batch_losses[idx_cls].mean()\n",
    "\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss += loss_value.item()\n",
    "        num_batches += 1\n",
    "        batch_predictions = torch.argmax(out, axis=-1)\n",
    "        preds.append(batch_predictions.cpu().numpy())\n",
    "\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "    preds = np.concatenate(preds, axis=0)\n",
    "    loss = loss / num_batches\n",
    "    report = {\n",
    "        'loss': loss,\n",
    "        'acc': metrics.accuracy_score(labels, preds),\n",
    "        'macro_f1': metrics.f1_score(labels, preds, average='macro', zero_division=0),\n",
    "        'macro_precision': metrics.precision_score(labels, preds, average='macro', zero_division=0),\n",
    "        'macro_recall': metrics.recall_score(labels, preds, average='macro', zero_division=0),\n",
    "        'micro_f1': metrics.f1_score(labels, preds, average='micro', zero_division=0),\n",
    "        'micro_precision': metrics.precision_score(labels, preds, average='micro', zero_division=0),\n",
    "        'micro_recall': metrics.recall_score(labels, preds, average='micro', zero_division=0),\n",
    "        'cls_report': metrics.classification_report(labels, preds, zero_division=0),\n",
    "        'cfm': metrics.confusion_matrix(labels, preds)\n",
    "    }\n",
    "    return report, q\n",
    "\n",
    "def run_train_gdro(net, optimizer, criterion, device, train_dl, val_dl, output_dir, max_epochs=30, n_classes=3):\n",
    "    num_epochs_ni = 0\n",
    "    best_vacc = 0\n",
    "    best_val_f1 = 0\n",
    "    logs = {'train':defaultdict(list), 'val':defaultdict(list)}\n",
    "    q = torch.ones(n_classes, dtype=torch.float32, device=device) / n_classes\n",
    "    \n",
    "    for epoch in tqdm(range(max_epochs)):\n",
    "        train_report, q = train_gdro(net, optimizer, device, criterion, train_dl, q=q)\n",
    "        for k in train_report.keys():\n",
    "            logs['train'][k].append(train_report[k])\n",
    "            \n",
    "        val_report = validate(net, device, criterion, val_dl)\n",
    "        for k in val_report.keys():\n",
    "            logs['val'][k].append(val_report[k])\n",
    "            \n",
    "        if val_report['macro_f1'] > best_val_f1:\n",
    "            torch.save(net.cpu().state_dict(), f'{output_dir}/model.pt')\n",
    "            best_val_f1 = val_report['macro_f1']\n",
    "    return logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42374c29-f343-494b-b850-a6ec07f75d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cls_embeddings(all_messages, model, tokenizer, device, m_length=96):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for subject_messages in tqdm(all_messages):\n",
    "            input = tokenizer(subject_messages, padding=True, truncation=True, max_length=m_length, return_tensors='pt')\n",
    "            output = model(**input.to(device))\n",
    "            embeddings.append(output.last_hidden_state[:, 0, :].cpu().numpy())\n",
    "    # embeddings = torch.vstack(embeddings)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5c58a7-3442-40b7-9ae8-6ba700395f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plot(train_scores, val_scores, y_label, figsize=(8,5)):\n",
    "    fig, ax = plt.subplots(1,1,figsize=figsize)\n",
    "    ax.plot(train_scores, label='Train')\n",
    "    ax.plot(val_scores, label='Val')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel(y_label)\n",
    "    ax.legend()\n",
    "\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0702a025-dccb-4520-9870-a6cb2aa96ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "textual_emoticons_to_spanish = {\n",
    "        \":)\": \"cara sonriente\",\n",
    "        \":(\": \"cara triste\",\n",
    "        \";)\": \"guiño\",\n",
    "        \":D\": \"cara riendo con los ojos abiertos\",\n",
    "        \"XD\": \"cara riendo con los ojos cerrados\",\n",
    "        \"xD\": \"cara riendo con los ojos cerrados\",\n",
    "        \":P\": \"cara sacando la lengua\",\n",
    "        \"<3\": \"corazón\",\n",
    "        \":'(\": \"cara llorando\",\n",
    "        \":-)\": \"cara sonriente\",\n",
    "        \":-(\": \"cara triste\",\n",
    "        \";-)\": \"guiño\",\n",
    "        \":-D\": \"cara riendo con los ojos abiertos\",\n",
    "        \":-P\": \"cara sacando la lengua\",\n",
    "        \"(heart)\": \"corazón\",\n",
    "        \":o\": \"cara sorprendida\",\n",
    "        \":-o\": \"cara sorprendida\",\n",
    "        \":/\": \"cara de duda\"\n",
    "    }\n",
    "\n",
    "def preprocess(text):\n",
    "    # print(text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'(ja)+', 'jaja', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'(js)+', 'jsjs', text, flags=re.IGNORECASE)\n",
    "    text = text.replace(\" @\",\"o\")\n",
    "    for x,y in textual_emoticons_to_spanish.items():\n",
    "        text = text.replace(x,y)\n",
    "    text = emoji.demojize(text,language='es')\n",
    "    spanishVowels = 'aeiouáéíóú'\n",
    "    uppercaseVowels =spanishVowels.upper()\n",
    "    for vow in spanishVowels + uppercaseVowels:\n",
    "        pattern = re.compile(f\"{vow}{vow}{vow}+\")\n",
    "        text = pattern.sub(f'{vow}',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57250fc0-395a-4941-bb82-8d266bfe5852",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cls_embeddings_2(messages, model, tokenizer, device, m_length=96):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    n_messages = len(messages)\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, n_messages, 128)):\n",
    "            msg_batch = messages[i: min(i+128, n_messages)]\n",
    "            input = tokenizer(msg_batch, padding=True, truncation=True, max_length=m_length, return_tensors='pt')\n",
    "            output = model(**input.to(device))\n",
    "            embeddings.append(output.last_hidden_state[:, 0, :].cpu().numpy())\n",
    "    \n",
    "    embeddings = np.concatenate(embeddings, axis=0)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3996cc-5b7f-40d1-987a-eec73b848de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data provided by the organizers for the first two tasks\n",
    "data_dir = '/path/to/data/dir'\n",
    "\n",
    "LABEL_MAP = {'none':0, 'anxiety':1, 'depression':2}\n",
    "REVERSE_LABEL_MAP = {0:'none', 1:'anxiety', 2:'depression'}\n",
    "\n",
    "CONTEXTS = ['addiction','emergency','family','work','social','other']\n",
    "CONTEXT_MAP = {c:i for i,c in enumerate(CONTEXTS)}\n",
    "CONTEXT_MAP['none'] = len(CONTEXTS)\n",
    "REVERSE_CONTEXT_MAP = {CONTEXT_MAP[c]:c for c in CONTEXT_MAP.keys()}\n",
    "\n",
    "data = {'messages':[], 'labels1':[], 'labels2':[], 'dates':[]}\n",
    "\n",
    "# read data and preprocess text messages\n",
    "for split in ['trial', 'train']:\n",
    "    df = pd.read_csv(os.path.join(data_dir, split, 'gold_task2.txt'))\n",
    "    labels1 = df['label'].map(lambda x: LABEL_MAP[x]).to_list()\n",
    "    data['labels1'] += labels1\n",
    "    \n",
    "    subjects = df['Subject'].to_list()\n",
    "    messages = []\n",
    "    dates = []\n",
    "    for subject in subjects:\n",
    "        subject_data = json.load(open(os.path.join(data_dir, split, 'subjects', f'{subject}.json') , 'r', encoding='utf-8'))\n",
    "        messages.append([preprocess(x['message']) for x in subject_data])\n",
    "        dates.append([x['date'] for x in subject_data])\n",
    "    data['messages'] += messages\n",
    "    data['dates'] += dates\n",
    "    data['labels2'].append(df[CONTEXTS].to_numpy().astype(np.int32))\n",
    "\n",
    "data['labels2'] = np.concatenate(data['labels2'], axis=0).astype(np.float32)\n",
    "data['labels1'] = np.array(data['labels1'], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1c6db0-a244-4d2c-bf9c-5a92bb77f388",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "labels = data['labels1']\n",
    "name = 'pysentimiento/robertuito-sentiment-analysis'\n",
    "tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "model = AutoModel.from_pretrained(name)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "msg_healthy = [x for x,y in zip(data['messages'], data['labels1']) if y == 0]\n",
    "embs = get_cls_embeddings(msg_healthy, model, tokenizer, device)\n",
    "embs_flat = np.concatenate(embs, axis=0) # embeddings of messages from individuals not suffering from depression or anxiety\n",
    "\n",
    "msg_other = [x for x,y in zip(data['messages'], data['labels1']) if y != 0]\n",
    "embs_other = get_cls_embeddings(msg_other, model, tokenizer, device)\n",
    "embs_other = np.concatenate(embs_other, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cf9970-a045-47a9-9487-210611af84de",
   "metadata": {},
   "outputs": [],
   "source": [
    "task3_data_dir = '/path/to/additional/data' \n",
    "# datasets downloded from https://huggingface.co/datasets/somosnlp-hackathon-2023/suicide-comments-es \n",
    "# and https://github.com/kvvaldez/spanish_suicide/tree/master\n",
    "\n",
    "data1 = pd.read_csv(os.path.join(task3_data_dir ,'suicide_comments_es.csv'))\n",
    "data2 = pd.read_csv(os.path.join(task3_data_dir ,'suicidio_notacion.csv'))\n",
    "\n",
    "pos_samples1 = data1.Text[data1.Label == 1].map(lambda x: preprocess(x)).to_list()\n",
    "pos_samples2 = data2.tweet_clean[data2.suicidio == 1].map(lambda x: preprocess(x)).to_list()\n",
    "\n",
    "pos_embs1 = get_cls_embeddings_2(pos_samples1, model, tokenizer, device=device, m_length=128)\n",
    "pos_embs2 = get_cls_embeddings_2(pos_samples2, model, tokenizer, device=device, m_length=128)\n",
    "pos_embs = np.concatenate((pos_embs1, pos_embs2), axis=0)\n",
    "\n",
    "all_embs = np.concatenate([embs_flat, pos_embs], axis=0)\n",
    "\n",
    "all_labels = np.zeros(all_embs.shape[0], dtype=np.int32)\n",
    "all_labels[embs_flat.shape[0]:] = 1\n",
    "assert np.sum(all_labels) == pos_embs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b702497-386d-4ec4-8ace-bac72081d30a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_dir = '/path/save/networks/'\n",
    "\n",
    "ds = EmbDataset(all_embs, all_labels)\n",
    "train_ds, test_ds = random_split(ds, [0.8, 0.2], generator=torch.Generator().manual_seed(1007))\n",
    "test_dl = DataLoader(test_ds, batch_size=128, shuffle=False, drop_last=False)\n",
    "\n",
    "f1s = []\n",
    "for batch_size in [32, 64, 128]:\n",
    "    scores = []\n",
    "    for lr_idx, lr in enumerate([5e-2, 1e-3, 5e-3]):\n",
    "        output_dir = os.path.join(save_dir, f'linear_preprocess_{batch_size}_{lr_idx}')\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        random.seed(1007)\n",
    "        np.random.seed(1007)\n",
    "        torch.manual_seed(1007)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        \n",
    "        train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "\n",
    "        net = nn.Linear(all_embs[0].shape[-1], 2, bias=True)\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        logs = run_train_gdro(net,optimizer,loss_fn,device, train_dl, test_dl, output_dir, max_epochs=50, n_classes=2)\n",
    "        for k in logs['train'].keys():\n",
    "            if k != 'cls_report' and k != 'cfm':\n",
    "                fig, ax = make_plot(logs['train'][k], logs['val'][k], k)\n",
    "                fig.savefig(f'{output_dir}/{k}.png')\n",
    "                plt.close(fig)\n",
    "        \n",
    "        np.save(f'{output_dir}/logs.npy', logs, allow_pickle=True)\n",
    "        arg = np.argmax(logs['val']['macro_f1'])\n",
    "        print(f\"batch_size: {batch_size}, lr: {lr}, arg: {arg}\")\n",
    "        print(f\"Val macro_f1: {logs['val']['macro_f1'][arg]:.4f} | Train macro_f1: {logs['train']['macro_f1'][arg]:.4f}\")\n",
    "        print('Val', logs['val']['cfm'][arg], logs['val']['cls_report'][arg], sep='\\n')\n",
    "        print('Train', logs['train']['cfm'][arg], logs['train']['cls_report'][arg], sep='\\n')\n",
    "        \n",
    "        scores.append(logs['val']['macro_f1'][arg])\n",
    "    f1s.append(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f3c1c0-e4e3-4018-be00-436ac596ec8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embs_flat_ = np.concatenate([embs_flat, embs_other], axis=0) # add the subjects suffering from depression or anxiety as negative examples (non-suicidal)\n",
    "\n",
    "all_embs = np.concatenate([embs_flat_, pos_embs], axis=0)\n",
    "\n",
    "all_labels = np.zeros(all_embs.shape[0], dtype=np.int32)\n",
    "all_labels[embs_flat.shape[0]:] = 1\n",
    "assert np.sum(all_labels) == pos_embs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a17b4f3-3aa7-42d1-abc9-97159a8c325b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_dir = '/path/save/networks'\n",
    "\n",
    "ds = EmbDataset(all_embs, all_labels)\n",
    "train_ds, test_ds = random_split(ds, [0.8, 0.2], generator=torch.Generator().manual_seed(1007))\n",
    "test_dl = DataLoader(test_ds, batch_size=128, shuffle=False, drop_last=False)\n",
    "\n",
    "f1s = []\n",
    "for batch_size in [32, 64, 128]:\n",
    "    scores = []\n",
    "    for lr_idx, lr in enumerate([5e-2, 1e-3, 5e-3]):\n",
    "        output_dir = os.path.join(save_dir, f'linear_preprocess_all_{batch_size}_{lr_idx}')\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        random.seed(1007)\n",
    "        np.random.seed(1007)\n",
    "        torch.manual_seed(1007)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        \n",
    "        train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "\n",
    "        net = nn.Linear(all_embs[0].shape[-1], 2, bias=True)\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        logs = run_train_gdro(net,optimizer,loss_fn,device, train_dl, test_dl, output_dir, max_epochs=50, n_classes=2)\n",
    "        for k in logs['train'].keys():\n",
    "            if k != 'cls_report' and k != 'cfm':\n",
    "                fig, ax = make_plot(logs['train'][k], logs['val'][k], k)\n",
    "                fig.savefig(f'{output_dir}/{k}.png')\n",
    "                plt.close(fig)\n",
    "        \n",
    "        np.save(f'{output_dir}/logs.npy', logs, allow_pickle=True)\n",
    "        arg = np.argmax(logs['val']['macro_f1'])\n",
    "        print(f\"batch_size: {batch_size}, lr: {lr}, arg: {arg}\")\n",
    "        print(f\"Val macro_f1: {logs['val']['macro_f1'][arg]:.4f} | Train macro_f1: {logs['train']['macro_f1'][arg]:.4f}\")\n",
    "        print('Val', logs['val']['cfm'][arg], logs['val']['cls_report'][arg], sep='\\n')\n",
    "        print('Train', logs['train']['cfm'][arg], logs['train']['cls_report'][arg], sep='\\n')\n",
    "        \n",
    "        scores.append(logs['val']['macro_f1'][arg])\n",
    "    f1s.append(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
